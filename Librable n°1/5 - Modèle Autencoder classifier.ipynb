{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Autoencoder Multi-Class__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classification par autoencoder fonctionne de la façon suivante :\n",
    "- Dans un premier temps, on utilise un encoder pour réduire les images et détecter les zones importantes.\n",
    "- Dans un second temps, on utilise un classificateur classique pour classer les images.\n",
    "\n",
    "Nous allons utiliser un autoencoder fully-connected, entrainer notre modèle et récupérer la partie encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alternative text](https://www.researchgate.net/publication/331230358/figure/fig4/AS:959315259162633@1605729989983/The-schematic-architecture-of-a-classifier-autoencoder-A-gene-expression-profile-GEP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Importation des bibliothèques__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import cv2\n",
    "import shutil\n",
    "from tensorflow import keras, losses\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Création des jeux de données d'entrainement, de validation et de test__\n",
    "\n",
    "À partir du jeu de données fourni de la phase de pré-processing, il nous suffit de créer les différents Tensor pour réaliser nos différents tests.\n",
    "\n",
    "Dans un premier temps, nous créons les jeux de données d'entrainement et de validation. Pour cela, nous nous rendons dans le dossier 'Multi-Class' de notre dataset et nous prenons le dossier 'Train'. Nous prenons ce dossier pour les deux jeux de données.\n",
    "\n",
    "Ensuite, nous devons faire un split de ces jeux de données. Pour cela, il faut utiliser le paramètre 'Validation_split'. Nous redimensionnons les images en 128 par 128 pixels, et nous donnons les subsets 'Training' et 'Validation'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1015 files belonging to 5 classes.\n",
      "Using 812 files for training.\n",
      "Found 1015 files belonging to 5 classes.\n",
      "Using 203 files for validation.\n"
     ]
    }
   ],
   "source": [
    "train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=r\"D:\\Quentin\\Etudes\\Cesi\\A5\\Option Data\\Projet\\Autoencoder\\dataset_reduit\",\n",
    "    labels=\"inferred\",\n",
    "    validation_split=0.2,\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=['Painting', 'Photo', 'Schematics', 'Sketch', 'Text'],\n",
    "    seed=41,\n",
    "    subset='training',\n",
    "    image_size=(128,128),\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    ")\n",
    "validate = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=r\"D:\\Quentin\\Etudes\\Cesi\\A5\\Option Data\\Projet\\Autoencoder\\dataset_reduit\",\n",
    "    labels=\"inferred\",\n",
    "    validation_split=0.2,\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=['Painting', 'Photo', 'Schematics', 'Sketch', 'Text'],\n",
    "    seed=41,\n",
    "    subset='validation',\n",
    "    image_size=(128,128),\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le jeu de données de test, il faut prendre le dossier 'test' dans le dossier 'Multi-class'. Nous n'avons pas besoin de séparer nos données ou de créer un subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8199 files belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "test = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=r\"D:\\Quentin\\Perso\\dataset\\multi-class\\test\",\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"categorical\",\n",
    "    class_names=['Painting', 'Photo', 'Schematics', 'Sketch', 'Text'],\n",
    "    seed=41,\n",
    "    subset=None,\n",
    "    image_size=(128,128),\n",
    "    interpolation=\"bilinear\",\n",
    "    follow_links=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Autoencoder__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Normalisation des données*\n",
    "Il est possible d'utiliser un autoencoder afin de faire une classification. Dans un premier temps, nous créons un encoder décodeur.\n",
    "\n",
    "Pour cela, il faut d'abords normaliser nos données. Cela nous permettra d'avoir un meilleur résultat.\n",
    "\n",
    "Nous utilisons la méthode Rescaling, dans le but de redimensionner toutes les données entre 0 et 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "def change_inputs(images, labels):\n",
    "  x = tf.image.resize(normalization_layer(images),[128, 128], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "  return x, x\n",
    "normalized_ds = train.map(change_inputs)\n",
    "normalized_validate = validate.map(change_inputs)\n",
    "normalized_test = test.map(change_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Création de la fonction d'Autoencoder*\n",
    "Ensuite, nous créons l'autoencoder. Il est composé de deux parties :\n",
    "- L'encoder, qui va permettre de réduire les images, et qui sera utile pour le classificateur.\n",
    "- Le décoder, qui sera utile pour l'entrainement du modèle. Il sera ensuite retiré lors des prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création de l'encoder\n",
    "Dans un premier temps, nous créons l'encoder. On commence par créer l'encoder.\n",
    "\n",
    "- Nous commençons par créer notre couche d'entrée, qui correspond au nombre de pixels de notre image.\n",
    "\n",
    "- Nous récupérons cette entrée pour faire un Flatten(), afin de mettre toutes les entrées sur une seule ligne.\n",
    "\n",
    "- Nous créons ensuite deux couches, une de 4 096 entrées avec une fonction d'activation 'RElu', et une couche de batchnormalization. Cette fonction permet de normaliser la sortie de la couche précédente afin de facilité l'apprentissage dans la couche suivante. Cette fonction de normalisation s'effectue sur chacun des batchs de notre jeu de données.\n",
    "\n",
    "- Nous terminons par créer notre couche d'espace latent, qui récupère les images réduites.\n",
    "\n",
    "À la fin de ces étapes, nous créons modèle d'encoder, avec comme entrée la variable 'encoder_inputs' et comme sortie la variable 'latent_space'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création du décoder\n",
    "La structure du décoder va ressembler à celui de l'encoder.\n",
    "\n",
    "- Nous récupérons la sortie de l'encoder, qui se trouve dans la variable 'latent_space' pour créer la couche d'entrée du décoder. Nous ne mettons que 64 comme taille d'entrée, car c'est la taille de la variable 'latent_space'.\n",
    "\n",
    "- Nous créons ensuite deux couches, une de 4 096 entrées avec une fonction d'activation 'RElu', et une couche de batchnormalization.\n",
    "\n",
    "- Nous créons une autre couche avec 49 152 entrées et nous utilisons la fonction d'activation 'sigmoid'. Utiliser cette fonction est plus efficace pour la recomposition de l'image.\n",
    "\n",
    "- Enfin, nous allons utiliser la fonction Reshape, afin de restaurer les images dans leur forme d'origine. Si nous ne faisons pas ce redimensionnement, nous ne pourrions pas faire la comparaison entre les images d'entrées et les images de sorties.\n",
    "\n",
    "À la fin de ces étapes, nous créons modèle du décoder, avec comme entrée la variable 'decoder_inputs' et comme sortie la variable 'output'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fusion de l'encoder et du décoder pour créer l'autoencoder\n",
    "Une fois que l'encoder et le décoder créés, nous utilisons la même méthode pour créer l'autoencoder, en utilisant comme variable d'entrée 'encoder_inputs' et comme variable de sortie 'outputs'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder(data_train, data_validation, nb_epochs, save_weights_path):\n",
    "    latent_space_dim = 64\n",
    "# Architecture de l'Encodeur\n",
    "    encoder_inputs = tf.keras.Input(shape=(128, 128, 3))\n",
    "    flatten = tf.keras.layers.Flatten()(encoder_inputs)\n",
    "    hidden3 = tf.keras.layers.Dense(4096, activation='relu')(flatten)\n",
    "    hidden4 =  tf.keras.layers.BatchNormalization()(hidden3)\n",
    "    latent_space = tf.keras.layers.Dense(latent_space_dim, activation='relu')(hidden4)\n",
    "\n",
    "    encoder = keras.Model(encoder_inputs, latent_space, name=\"encoder\")\n",
    "\n",
    "# Architecture du Décodeur  \n",
    "    decoder_inputs = tf.keras.Input(shape=(64,))\n",
    "    hidden9 = tf.keras.layers.Dense(4096, activation='relu')(decoder_inputs)\n",
    "    hidden10 = tf.keras.layers.BatchNormalization()(hidden9)\n",
    "    decoder_outputs = tf.keras.layers.Dense(49152, activation='sigmoid')(hidden10)\n",
    "    output = tf.keras.layers.Reshape(target_shape=(128, 128 ,3))(decoder_outputs)\n",
    "\n",
    "    decoder = keras.Model(decoder_inputs, output, name=\"decoder\")\n",
    "\n",
    "# Combinaison des deux Architecture (Enc,Dec)\n",
    "    outputs = decoder(latent_space)\n",
    "    autoencoder = keras.Model(encoder_inputs, outputs, name=\"autoencoder\")\n",
    "    autoencoder.summary()\n",
    "\n",
    "# Compilation du modèle\n",
    "    autoencoder.compile(optimizer='adam', loss= 'mse', metrics=['accuracy'])\n",
    "\n",
    "# Exécution du modèle\n",
    "    history = autoencoder.fit(data_train, epochs=nb_epochs,\n",
    "                    batch_size=32,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(data_validation))\n",
    "\n",
    "# Sauvegarde des paramètres\n",
    "    autoencoder.save(r\"D:\\Quentin\\Etudes\\Cesi\\A5\\Option Data\\Projet\\Autoencoder\\poids\\model_deux.h5\")\n",
    "    encoder.save(r\"D:\\Quentin\\Etudes\\Cesi\\A5\\Option Data\\Projet\\Autoencoder\\poids\\encoder_deux.h5\")\n",
    "\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilation\n",
    "Lors de la Compilation du modèle, nous utilisons l'optimizer 'Adam' ainsi que la fonction de perte MSE. Le but de la MSE, où Mean Squared Error, est de calculer le carré de la différence entre la valeur prédite et la valeur réel. Nous faisons ensuite une moyenne sur ces valeurs. Nous utilisons la MSE comme fonction perte, car elle apporte de bons résultats avec des données comme les images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apprentissage\n",
    "Enfin vient le moment de l'entrainement du modèle. Pour cela, dans les paramètres de la fonction fit, nous définissons la taille des batchs à 32. Nous définissons plus loin le nombre d'époques à utiliser, qui est de 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde des poids\n",
    "Nous terminons cette méthode de création d'un autoencoder par l'enregistrement des poids. Dans un premier temps, nous enregistrons l'ensemble des poids de l'autoencoder. Il pourrait nous servir lors de la phase de traitement d'images ou pour d'autres projets. Il peut vous être utile afin d'éviter de lancer tout le code.\n",
    "\n",
    "Enfin, nous enregistrons les poids de la partie encoder. C'est cette partie qui nous intéresse pour la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"autoencoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 49152)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              201330688 \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 4096)             16384     \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                262208    \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 128, 128, 3)       201658368 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 403,267,648\n",
      "Trainable params: 403,251,264\n",
      "Non-trainable params: 16,384\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "26/26 [==============================] - 55s 2s/step - loss: 0.0758 - accuracy: 0.3074 - val_loss: 0.4643 - val_accuracy: 0.3229\n",
      "Epoch 2/10\n",
      "26/26 [==============================] - 50s 2s/step - loss: 0.0536 - accuracy: 0.3234 - val_loss: 0.3281 - val_accuracy: 0.3233\n",
      "Epoch 3/10\n",
      "26/26 [==============================] - 52s 2s/step - loss: 0.0418 - accuracy: 0.3499 - val_loss: 0.0961 - val_accuracy: 0.2834\n",
      "Epoch 4/10\n",
      "26/26 [==============================] - 51s 2s/step - loss: 0.0337 - accuracy: 0.4433 - val_loss: 0.0778 - val_accuracy: 0.3110\n",
      "Epoch 5/10\n",
      "26/26 [==============================] - 48s 2s/step - loss: 0.0331 - accuracy: 0.4280 - val_loss: 0.0547 - val_accuracy: 0.4791\n",
      "Epoch 6/10\n",
      "26/26 [==============================] - 48s 2s/step - loss: 0.0335 - accuracy: 0.4598 - val_loss: 0.0522 - val_accuracy: 0.4731\n",
      "Epoch 7/10\n",
      "26/26 [==============================] - 49s 2s/step - loss: 0.0330 - accuracy: 0.4289 - val_loss: 0.0327 - val_accuracy: 0.5458\n",
      "Epoch 8/10\n",
      "26/26 [==============================] - 47s 2s/step - loss: 0.0309 - accuracy: 0.4238 - val_loss: 0.0401 - val_accuracy: 0.6418\n",
      "Epoch 9/10\n",
      "26/26 [==============================] - 51s 2s/step - loss: 0.0273 - accuracy: 0.4559 - val_loss: 0.0434 - val_accuracy: 0.6532\n",
      "Epoch 10/10\n",
      "26/26 [==============================] - 48s 2s/step - loss: 0.0298 - accuracy: 0.4633 - val_loss: 0.0476 - val_accuracy: 0.5794\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ae = create_autoencoder(normalized_ds, normalized_validate, 10, \"D:\\\\Quentin\\\\Etudes\\\\Cesi\\\\A5\\\\Option Data\\\\Projet\\\\Autoencoder\\\\poids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interprétation des résultats\n",
    "Nous nous rendons compte, au vu du graphique ci-dessus, que notre autoencoder est assez performant. En effet, nous pouvons voir que notre modèle arrive à généraliser sur les données. Mais ce test n'a pas été réalisé qu'avec un échantillon de 1000 images.\n",
    "\n",
    "Ce modèle est en réalité assez peu performant, ce qui est principalement du au nombre limité de couches dans notre réseau. En effet, nous ne pouvions pas mettre beaucoup de couches avec une réduction et une augmentation rapide des images, car le nombre de paramètres serait trop élevé et le temps d'entrainement de notre modèle aurait été trop élevé.\n",
    "\n",
    "Une autre conséquence de cette performance est l'aspect Fully-Connected de notre réseau. Cela augmente le nombre de paramètres, et donc le temps d'entrainement du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code pour charger les poids d'un modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saved_model = tf.keras.models.load_model(r\"D:\\Quentin\\Etudes\\Cesi\\A5\\Option Data\\Projet\\Autoencoder\\poids\\model_un.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Classifier*\n",
    "Le classifier est la partie qui va permettre de classer les images encodées. La structure du modèle est de la forme suivante :\n",
    "- Nous commençons par faire un Flatten() en récupérant la sortie de l'encoder.\n",
    "- Ensuite, nous faisons 4 couches de neurones de 64, 32, 16 et 5, où 5 est le nombre de classes de nos images.\n",
    "- Nous compilons le modèle avec l'optimizer 'adam' et la fonction de perte 'SparseCategoricalCrossentropy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.keras.Sequential()\n",
    "classifier.add(tf.keras.layers.Flatten(input_shape=(812, 64)))\n",
    "classifier.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "classifier.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "classifier.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "classifier.add(tf.keras.layers.Dense(5, activation='relu'))\n",
    "classifier.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Problèmes rencontrés*\n",
    "L'autoencoder Classifier n'est pas encore terminé. Nous rencontrons des difficultés avec la taille de la sortie de l'encoder avec l'entrée du classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n",
      "26/26 [==============================] - 6s 134ms/step\n",
      "7/7 [==============================] - 2s 126ms/step\n"
     ]
    }
   ],
   "source": [
    "encoder = tf.keras.models.load_model(r\"D:\\Quentin\\Etudes\\Cesi\\A5\\Option Data\\Projet\\Autoencoder\\poids\\encoder_deux.h5\")\n",
    "X_train_encoded = encoder.predict(normalized_ds)\n",
    "Y_train_encoded = encoder.predict(normalized_validate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Quentin\\Etudes\\Cesi\\A5\\Option Data\\Projet\\main.ipynb Cellule 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Quentin/Etudes/Cesi/A5/Option%20Data/Projet/main.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(X_train_encoded, validation_data\u001b[39m=\u001b[39;49m(Y_train_encoded),\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Quentin/Etudes/Cesi/A5/Option%20Data/Projet/main.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m             epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Quentin/Etudes/Cesi/A5/Option%20Data/Projet/main.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m             batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Quentin/Etudes/Cesi/A5/Option%20Data/Projet/main.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Le Fortier\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Le Fortier\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\keras\\engine\\training.py:1483\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[39mif\u001b[39;00m validation_split \u001b[39mand\u001b[39;00m validation_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1473\u001b[0m     \u001b[39m# Create the validation data using the training data. Only supported\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m     \u001b[39m# for `Tensor` and `NumPy` input.\u001b[39;00m\n\u001b[0;32m   1475\u001b[0m     (\n\u001b[0;32m   1476\u001b[0m         x,\n\u001b[0;32m   1477\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1480\u001b[0m         (x, y, sample_weight), validation_split\u001b[39m=\u001b[39mvalidation_split\n\u001b[0;32m   1481\u001b[0m     )\n\u001b[1;32m-> 1483\u001b[0m \u001b[39mif\u001b[39;00m validation_data:\n\u001b[0;32m   1484\u001b[0m     (\n\u001b[0;32m   1485\u001b[0m         val_x,\n\u001b[0;32m   1486\u001b[0m         val_y,\n\u001b[0;32m   1487\u001b[0m         val_sample_weight,\n\u001b[0;32m   1488\u001b[0m     ) \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39munpack_x_y_sample_weight(validation_data)\n\u001b[0;32m   1490\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39m_should_use_with_coordinator:\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train_encoded, validation_data=(Y_train_encoded),\n",
    "            epochs=5,\n",
    "            batch_size=32,\n",
    "            shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7efb1a62b5e4d6a1eb98494641a36180ec477bc661a6e1e9b0fe4f65a1b72ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
